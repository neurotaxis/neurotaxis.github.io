<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>How to think in ten thousand dimensions</title>
    <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {inlineMath: [['\\(','\\)']], displayMath: [['\\[','\\]']]},
    "HTML-CSS": { 
      linebreaks: { automatic: true, width: "90% container" },
      scale: 100
    },
    SVG: { linebreaks: { automatic: true } }
  });
</script>
    <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>
  <link rel="stylesheet" href="/style.css">
</head>
<body>
  <div class="container">
    <div id="header-placeholder"></div>

    <main>
        
      <section class="post">
          <div class="post-content">
            <h1>How to think in ten thousand dimensions</h1>
<p><strong>Rich Pang</strong></p>
<p>2026-01-05</p>
<p>Many neuroscience and AI models operate in high-dimensional (HD) spaces, for instance with each dimension corresponding to the activation of a neuron.
HD vectors have many interesting and useful properties that can appear rather counterintuitive.</p>
<p>In HD, for example, one can encode a set by summing or averaging its elements together.
In low dimensions averaging throws away a lot of information:
if one replaced the set <span id="math-equation">\(\{3, 10, 2\}\)</span> with its average, 5, it would be hard to decode or recover the original three elements from the number 5.
In HD spaces such averaging can in fact preserve the original elements, which may seem surprising.</p>
<p>Below are two tricks to help make HD vectors more intuitive and predictable to think about. They are in fact quite related and meant to reflect dense and sparse, or "L2" and "L1" settings, which are important in both neuroscience and AI.</p>
<h2>Imagine high-dimensional vectors as images</h2>
<p>That sets can be represented as averages of their high-dimensional (HD) vector representations, or codewords, can be either derived mathematically or understood intuitively by thinking of the codewords as images.</p>
<p>Mathematically, random HD vectors are almost orthogonal with very high probability:</p>
<p>\[\mathbf{r}_1 \cdot \mathbf{r}_2 \approx 0\]</p>
<p>when the individual elements of <span id="math-equation">\(\mathbf{r}_1\)</span> and <span id="math-equation">\(\mathbf{r}_2\)</span> are i.i.d.
This can be shown with a bit of algebra.
It can also be shown that it is possible to sample an exponentially large number <span id="math-equation">\(\exp(cN)\)</span> of such codewords, where <span id="math-equation">\(N\)</span> is the dimensionality of <span id="math-equation">\(\mathbf{r}_i\)</span>, although the proof is more complex.
This is quite qualitatively different from the number of exactly orthogonal codewords, which is just <span id="math-equation">\(N\)</span>.
There are many more than <span id="math-equation">\(N\)</span> HD codewords if near-orthogonality suits one's purposes.</p>
<p>Construct a set of codewords in this way, <span id="math-equation">\(\mathbf{r}_1 \dots \mathbf{r}_K\)</span>, corresponding to some set of items <span id="math-equation">\(1\)</span> through <span id="math-equation">\(K\)</span>, and with the codewords normalized such that <span id="math-equation">\(\mathbf{r}_i \cdot \mathbf{r}_i \approx 1\)</span>. Pick items <span id="math-equation">\(k, l, m\)</span> and average their codewords:</p>
<p>\[\mathbf{r}_{avg} = \frac{1}{3}\left(\mathbf{r}_k + \mathbf{r}_l + \mathbf{r}_m \right).\]
To verify that this quantity stores the set <span id="math-equation">\(\{k, l, m\}\)</span> we check codeword <span id="math-equation">\(\mathbf{r}_{k'}\)</span> for <span id="math-equation">\(k' \in \{k, l, m\}\)</span> and <span id="math-equation">\(k' \notin \{k, l, m\}\)</span>. We perform the check by taking the dot product <span id="math-equation">\(\mathbf{r}_{k'} \cdot \mathbf{r}_{avg}\)</span>.</p>
<p>In the former case, suppose <span id="math-equation">\(k' = k\)</span>. Then we have</p>
<p>\[\begin{split}
\mathbf{r}_{k} \cdot \mathbf{r}_{avg} 
& = \frac{1}{3}\left(\mathbf{r}_{k} \cdot \mathbf{r}_k + \mathbf{r}_{k} \cdot \mathbf{r}_l + \mathbf{r}_{k} \cdot \mathbf{r}_m \right) \\
& \approx \frac{1}{3} + 0 + 0 = \frac{1}{3} \\
\end{split}\]</p>
<p>In the latter case:</p>
<p>\[\begin{split}
\mathbf{r}_{k'} \cdot \mathbf{r}_{avg} 
& = \frac{1}{3} \left(\mathbf{r}_{k'} \cdot \mathbf{r}_k + \mathbf{r}_{k'} \cdot \mathbf{r}_l + \mathbf{r}_{k'} \cdot \mathbf{r}_m \right)\\
& \approx 0 + 0 + 0 = 0 \\
\end{split}\]
Hence, whether the dot product is different from zero reveals whether any test element is in the subset represented by <span id="math-equation">\(\mathbf{r}_{avg}\)</span>, with an accuracy that depends on how many elements are in the set.</p>
<p>While the algebra above is simple and elegant, consider the following image, which is itself the superposition of three famous images.
We have decreased each original image's opacity such that each resulting pixel is an average of the corresponding pixels in each image.</p>
<p><img alt="Superposition of three famous images: Einstein sticking his tongue out, Pink Floyd's Dark Side of the Moon album cover, and Van Gogh's Starry Night" src="how_to_think_in_ten_thousand_dimensions_image_superposition.png" /></p>
<p>The average clearly retains the identities of the original images in some meaningful sense.
Images that are not in the superposition have low overlap with it, and images that are in the superposition have high overlap.
Curiously, whereas decoding the elements from the image in the algebraic manner described above requires checking each element against <span id="math-equation">\(\mathbf{r}_{avg}\)</span>, our brains seem to be able to decode the superimposed images quite quickly.</p>
<h2>Think of sparse binary vectors as one-hot vectors in an even higher dimensional space</h2>
<p>Sparse binary HD vectors are useful when non-negativity and finite resolution needed, e.g. when mapping codewords to binary memory elements in a computer or neural "spike words".
Although such vectors distribute information over multiple elements of the vector (this is one of their benefits, in fact), they can often be intuitied about by imagining them as 1-hot vectors in an even higher dimensional space.
In both cases, all elements are positive, different codewords are orthogonal (or nearly orthogonal), and the codewords are sparse, with most elements zero.</p>
<p>This perspective illuminates the basic action of a Bloom filter, for example.
Bloom filters are data structures used for set membership querying (e.g. to test whether a URL is in a set of previously visited URLs). 
They operate by mapping each element (URL) to a collection of <span id="math-equation">\(Q\)</span> bits in an array of size <span id="math-equation">\(B\)</span> using a hash function, such that the bits assigned to each element are a pseudorandom deterministic function of the element.
Elements are added to the BF by setting their bits in the array to 1.
Elements are decoded by comparing the codeword for an element with the BF itself.
If fewer than <span id="math-equation">\(Q\)</span> bits match, the element is not in the BF.
If all <span id="math-equation">\(Q\)</span> bits match, the element may be in the BF, with a false positive rate that depends on how many elements have been stored.</p>
<p>Our analogy lets us think of the element codewords, which are <span id="math-equation">\(Q\)</span>-hot vectors in a <span id="math-equation">\(B\)</span>-dimensional space, as 1-hot vectors in an <span id="math-equation">\(\sim \exp{cB}\)</span> dimensional space.
In turn, storing elements corresponds to looking up the index "assigned" to the element in the <span id="math-equation">\(\sim \exp{cB}\)</span> dimensional space and setting its bit to 1.
Querying the BF then amounts to computing the codeword for a test element and checking whether the bit at its address in the higher dimensional space is set to 1.</p>
<p><img alt="diagram of Bloom filter as summing 1-hot vectors in a very HD space" src="how_to_think_in_ten_thousand_dimensions_bloom_filter.png" /></p>
<p>Thus, sparse binary HD vectors receive the benefits of dimensionality yet act in many ways like 1-hot vectors. The correspondence breaks down when the BF inevitably "fills up", but it does so gracefully. The above picture makes intuiting about variations of BFs simpler as well, for instance if elements decay over time. The image analogy discussed above can also be relevant to Bloom filters. </p>
<p>Many brain areas are also thought to exhibit sparse HD neural codes, for instance in the hippocampus or insect mushroom body, which are thought to reflect the results of pattern-separation or decorrelation operations. As we have seen, such codes can behave similarly to 1-hot codes in higher dimensional spaces.</p>
<h2>Further reading</h2>
<ul>
<li><a href="https://rctn.org/vs265/kanerva09-hyperdimensional.pdf">Kanerva 2009</a></li>
<li><a href="https://www.chrismusco.com/amlds2025/lectures/lec4.pdf">Musco lecture notes</a></li>
<li><a href="https://arxiv.org/pdf/2101.05841">Wegner lecture notes</a></li>
<li><a href="https://en.wikipedia.org/wiki/Bloom_filter">Bloom filter Wikipedia Page</a></li>
<li><a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC7028396/">Sparse high-dimensionsal codes in the brain</a></li>
</ul>
          </div>
      </section>
        
    </main>

    <div id="footer-placeholder"></div>

  <script>
    // Simple header/footer inclusion
    fetch('/header.html')
      .then(response => response.text())
      .then(data => document.getElementById('header-placeholder').innerHTML = data);

    fetch('/footer.html')
      .then(response => response.text())
      .then(data => document.getElementById('footer-placeholder').innerHTML = data);

    // Hamburger menu toggle
    document.addEventListener('click', function(e){
      if(e.target.closest('.hamburger')){
        document.querySelector('.menu').classList.toggle('open');
      }
    });
  </script>
  </div>
</body>
</html>
