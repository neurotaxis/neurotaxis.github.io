<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Downloading large files from Dryad</title>
  <link rel="stylesheet" href="/style.css">
</head>
<body>
  <div class="container">
    <div id="header-placeholder"></div>

    <main>
        
      <section class="post">
          <div class="post-content">
        
        <h1>Downloading large files from Dryad</h1>
          <strong>Rich Pang</strong><br />2025-09-27
          
          <p>
          <a href="https://datadryad.org/">Dryad</a> is one of the major online repositories for hosting biological research data, and which makes it easy to download files just by clicking the download link. However, downloading large files (e.g. 100Gb) can be tricky, since limited bandwidth can yield hours-long downloads, which can easily get canceled if there is any interruption in network connectivity. 
          </p>
          <p>
              To avoid these problems, do the following.
          </p>
          
          <h2>Get the actual file URL</h2>
          <p>
          If you right-click on a Dryad file and select "Copy Link" you will probably receive something like <code>https://datadryad.org/downloads/file_stream/4111789</code>. Unfortunately this is not the proper URL of the actual data file, but rather a link that triggers a series of redirects to the actual file (hosted on Amazon S3). Thus, you will get an error if you try to paste it into a command-line downloading utility.
          </p>
          <p>
              To get the correct link:
              <ol>
                  <li>Start the download by clicking on the data file as usual.</li>
                  <li>Go to your download manager and cancel the download.</li>
                  <li>Right-click on the download in the download manager and click "Copy Download Link". You should get something long and comlicated, e.g. of the form <code>https://dryad-assetstore-merritt-west.s3.uw-west.....623514842c</code>. </li>
                  <li>Copy <em>this</em> link into your clipboard.</li>
                  
              </ol>
          </p>
          <h2>Download the file using the command line</h2>
          <p>There are various utilities you can use to download the file. You want something that will resume if the network connection is interrupted.</p>
              
          <h3>Download using <code>curl</code></h3>
          <p><code>curl</code> is a built-in command-line tool for downloading files from the internet. To download your data file with <code>curl</code>, use the following command (one line):</p>
          <p> <code>
              curl -L -C - "https://dryadassetstore-very-long-url" -o local_file_name_to_save_to.zip
              </code>
          </p>
          <p>
              Note: make sure to include the <code>-L -C -</code> flags to ensure the download will resume after network connectivity interruptions.
          </p>
              
          <h3>Download using <code>aria2</code></h3>
          <p>If you do not have aria2 installed you can install it with
              <code>
              sudo apt install aria2
              </code>
              on linux, or <code>brew install aria2</code> on Mac (note that you will first need to <a href="https://brew.sh/">install <code>brew</code></a>, an auxiliary package manager for Mac). After aria2 has been installed, run the command:
          </p>

          <p>
          <code>
          aria2c -x 16 -s 16 -k 1M "https://dryadassetstore-very-long-url" -o local_file_name_to_save_to.zip   
          </code>
          </p>
          <p>
              The flags here help decompose and parallelize the download for faster download speeds. More information about using <code>aria2</code> is available <a href="https://aria2.github.io/">here</a>.
          </p>

          <p>
              Both of these commands will download the full data file. It may still take a while due to bandwidth limitations on Dryad downloads, so leave your local machine powered on until the download has finished. However, these commands will ensure that the download continues through network interruptions.
          </p>

          <p>
              Note that if the connection is interrupted for a long time (several minutes), this might end the downloading process. However, the partial download and metadata for continuing it are saved by aria, so just run the exact same command again to pick up the download where it left off.
          </p>
              
          </div>
      </section>
        
    </main>

    <div id="footer-placeholder"></div>

  <script>
    // Simple header/footer inclusion
    fetch('/header.html')
      .then(response => response.text())
      .then(data => document.getElementById('header-placeholder').innerHTML = data);

    fetch('/footer.html')
      .then(response => response.text())
      .then(data => document.getElementById('footer-placeholder').innerHTML = data);

    // Hamburger menu toggle
    document.addEventListener('click', function(e){
      if(e.target.closest('.hamburger')){
        document.querySelector('.menu').classList.toggle('open');
      }
    });
  </script>
  </div>
</body>
</html>
