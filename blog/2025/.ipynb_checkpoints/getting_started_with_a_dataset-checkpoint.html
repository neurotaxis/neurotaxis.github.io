<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Getting started with a dataset</title>
  <link rel="stylesheet" href="/style.css">
</head>
<body>
  <div class="container">
    <div id="header-placeholder"></div>

    <script>
    fetch('/header.html')
      .then(response => response.text())
      .then(data => {
        document.getElementById('header-placeholder').innerHTML = data;
      })
      .catch(error => console.error('Error loading header:', error));
    </script>

    <main>
        
      <section class="post">
        
        <h1>Getting started with a dataset</h1>
          <strong>Rich Pang</strong><br />2025-07-14
          <p>
          Have a dataset in your hands you're not sure what to do with? It's great to follow your intuition, but if you find yourself running in circles, try the following to expedite your journey.
          </p>
          
          <h2>Build a mental model of the data</h2>
          The better your mental model of the data, the easier it is to simulate analyses in your head, at the bus stop or in the shower, to see which ones are more or less likely to produce something interesting vs hit dead ends.
          <h4>Plot of bunch of typical and atypical examples in your data.</h4>
          <p>Knowing what your raw (or lightly pre-processed) data just <em>looks like</em> is crucial for interpreting possibly complex analysis results. Figure out what examples represent "typical" system behavior and which "atypical" examples are likely caused by artifacts vs something truly interesting. Make sure there is clear signal in the data, or if not, that you have strong reason for believing a clear signal can be extracted with the proper analysis technique.</p>
          <h4>Learn as much as you can about how the data was collected</h4>
          <p>Datasets hide a ton of experimental context.
          The better you understand the experimental context the better you will understand your data.</p>
          <h4>Compute all the basic stats</h4>
          <p>
          Before you try anything sophisticated, compute all the basic histograms and correlations.
          How many trials are there? How many timepoints per trial?
          How big are the signals you're looking at?
          What are the basic timescales of the system?
          Which features are "real" vs artifactual? If it seems like it might be interesting to apply sophisticated technique XYZ, don't do it yet, but write down the idea and why it might be interesting.</p>
          <h4>Make an executive summary of the data</h4>
          <p>
          Collect all your data examples, histograms, and correlations, as well as all your notes about crucial experimental context. 
          Write down everything that's an obvious artifact or confound.
          Save it all as a PDF, then print it out.
          This is your basic guide to the data that will serve as reference for yourself and anyone else who wants to work with the data later.
          </p>
          <h4>Internalize the executive summary</h4>
          <p>Memorize all the most important numbers and obvious features of the data.</p>
        
          <h2>Write down everything obviously interesting</h2>
          Before you do anything else quantitative, make a list of everything that seems interesting about the data.
          <h4>Interesting means not predictable</h4>
          <p>
          Either via your own intuition or according to existing literature and dominant scientific thinking.
          When it comes to publishing, it's the way of thinking in the field that should be taken into account. 
          It's quite possible for you to think some feature of your data is completely sensible, but the field thinks otherwise.
          Spend time reading and getting up to speed about what typical experts in your field would expect to see in the data.
          Sometimes the literature doesn't reflect all the field's expectations, so watch talks (or talk to experts, if possible) to find out how people are really thinking beyond what they write in their papers.
          </p>
          <h4>Make predictions about your data</h4>
          <p>Go through your every element executive summary again making predictions about what an expert in the field would expect to see, and comparing your summary to those predictions.
          Would the field predict that typical data examples look the way they do?
          What would the field predict the extent and shapes of the histograms to look like?
          What correlations would the field predict to see?
          For each prediction, note whether it holds up or is violated by the data.
          The violations are the interesting bits, so long as they're not caused by uninteresting confounds.
          </p>
        <h4>Data can be made interesting through model comparison</h4>
          <p>If there's nothing obviously interesting (unlikely), either select a new dataset, take a model comparison approach.
              Sometimes the data is high quality but the most interesting phenomena are hidden below the surface.
              A tried-and-true approach to making data interesting is to identify two mutually exclusive models that are both reasonable in the field, and ask which one better explains the data (or which models explain which aspects of the data).
              This allows you to "triangulate" where the data live in a space of models.
          </p>
          
        <h2>Quantify the interesting phenomena</h2>
          The easiest way forward is to find something obviously interesting in the data, and then just be quantitative about it.
          <h4>Be as simple as possible</h4>
          <p>New questions may arise as the process continues. It is iterative</p>
          <p>Write about why your results are interesting. Explain why they were not predictable, or how they have created new evidence for ruling out certain models of your system.</p>

        <h2>Beware the twilight zone</h2>
          <p>
          The twilight zone of data analysis is when one applies a complex "reduction" method to the data and tries to interpret the results. While this can sometimes be sound and powerful, it is also easy to mistake a feature of the data for a feature of the reduction method. For instance, quantities like "number of clusters" are often a consequence of a parameter in the clustering algorithm (e.g. specifying cluster size), rather than a reflection of the best guess "true" number of clusters in a dataset. The best way to verify results obtained in the twilight zone is to run the same methods on artificial datasets, one in which the same result is expected to appear and one where it is not. Such anchor points will give the results applied to your data much more meaning and soundness.
          </p>
      </section>
        
    </main>

    <div id="footer-placeholder"></div>

    <script>
      fetch('/footer.html')
        .then(response => response.text())
        .then(data => {
          document.getElementById('footer-placeholder').innerHTML = data;
        })
        .catch(error => console.error('Error loading footer:', error));
    </script>
  </div>
</body>
</html>
