<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Getting started with a new dataset</title>
  <link rel="stylesheet" href="../../style.css">
</head>
<body>
  <div class="container">
    <div id="header-placeholder"></div>

    <script>
    fetch('../../header.html')
      .then(response => response.text())
      .then(data => {
        document.getElementById('header-placeholder').innerHTML = data;
      })
      .catch(error => console.error('Error loading header:', error));
    </script>

    <main>
        
      <section class="sidekick">
        
        <h1>Getting started with a new dataset</h1>
          (draft) 2025-05-31
          <h3>Build a mental model of the data</h3>
        <p>Compute all the basic stats.</p>
        <p>Write out down all obvious artifacts about your data to keep in mind.</p>
        <p>Spend some serious time just looking at it. Don't do anything fancy.</p>
        <p>Create an executive summary of your data where you combine all the basic stats, artifacts, and sanity checks. Save it as a PDF.</p>
          <h3>Write down everything obviously interesting about your data</h3>
          <p>Interesting means not predictable, either via intuition or according to existing literature.</p>
        <p>If there's nothing obviously interesting (unlikely), either select a new dataset, or begin a model comparison approach.</p>
        <p>Data can be made interesting by comparing models against it. When two models are mutually exclusive yet both reasonable according to the literature, identifying which model captures the data better, or certain features of the data better, is interesting.</p>
        <h3>Quantify the interesting phenomena</h3>
          <p>Be simple.</p>
          <p>New questions may arise as the process continues. It is iterative</p>
          <p>Write about why your results are interesting. Explain why they were not predictable, or how they have created new evidence for ruling out certain models of your system.</p>

        <h3>Beware the twilight zone</h3>
          The twilight zone of data analysis is when one applies a complex "reduction" method to the data and tries to interpret the results. While this can sometimes be sound and powerful, it is also easy to mistake a feature of the data for a feature of the reduction method. For instance, quantities like "number of clusters" are often a consequence of a parameter in the clustering algorithm (e.g. specifying cluster size), rather than a reflection of the best guess "true" number of clusters in a dataset. The best way to verify results obtained in the twilight zone is to run the same methods on artificial datasets, one in which the same result is expected to appear and one where it is not. Such anchor points will give the results applied to your data much more meaning and soundness.
      </section>
        
    </main>

    <footer>
      <p>&copy; 2025 Rich Pang. All rights reserved.</p>
    </footer>
  </div>
</body>
</html>
