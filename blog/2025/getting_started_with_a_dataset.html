<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Getting started with a dataset</title>
  <link rel="stylesheet" href="/style.css">
</head>
<body>
  <div class="container">
    <div id="header-placeholder"></div>

    <script>
    fetch('/header.html')
      .then(response => response.text())
      .then(data => {
        document.getElementById('header-placeholder').innerHTML = data;
      })
      .catch(error => console.error('Error loading header:', error));
    </script>

    <main>
        
      <section class="post">
        
        <h1>Getting started with a dataset</h1>
          <strong>Rich Pang</strong><br />2025-07-14
          
          <p>
          Have a dataset in your hands you're not sure what to do with? It's great to follow your nose, but if you find yourself running in circles, the following may help expedite your journey.
          </p>
          
          <h2>Build a mental model of the data</h2>
          The better your mental model of the data, the easier it is to simulate analyses in your head, at the bus stop or in the shower, to see which ones are more or less likely to produce something interesting vs hit dead ends.
          
          <h4>Plot of bunch of typical and atypical examples in your data.</h4>
          <p>Knowing what your raw (or lightly pre-processed) data just <em>looks like</em> is crucial for interpreting downstream analysis results. Which examples represent "typical" system behavior that would help someone else get a feel for what you're working with? Which "atypical" examples are likely caused by artifacts vs something truly interesting? Make sure there is clear signal in the data, or if not, that you have strong reason for believing a clear signal can be extracted with the proper methods.</p>
          
          <h4>Learn as much as you can about how the data was collected</h4>
          <p>Datasets hide a ton of experimental context.
          The better you understand the experimental context the better you will understand your data.</p>
          
          <h4>Compute all the basic stats</h4>
          <p>
          Before you do anything sophisticated, compute all the basic histograms and correlations.
          How many trials are there? How many timepoints per trial?
          How big are the signals you're looking at?
          What are the basic timescales of the system?
          Which features are real vs artifacts? If you feel the urge to do anything remotely sophisticated, don't do it yet, but write down the idea and why it might be interesting.</p>
          
          <h4>Make an executive summary of the data</h4>
          <p>
          Collect all your data examples, histograms, and correlations, as well as all your notes about crucial experimental context. 
          Write down everything that's an obvious artifact or confound.
          Save it all as a PDF, then print it out.
          This is your basic guide to the data that will serve as reference for yourself and anyone else who wants to work with the data later.
          </p>
          
          <h4>Internalize the executive summary</h4>
          <p>Memorize all the most important numbers and obvious features of the data.</p>
        
          <h2>Write down everything obviously interesting</h2>
          Before you do anything else quantitative, make a list of everything that seems interesting about the data.
          
          <h4>Interesting means not predictable</h4>
          <p>
          Either via your own intuition or according to existing literature and dominant scientific thinking.
          When it comes to publishing, it's the way of thinking in the field that should set the stage. 
          It's quite possible for you to think some feature of your data is completely sensible, but the field thinks otherwise.
          Spend time reading and getting up to speed about what typical experts in your field would expect to see in the data.
          Sometimes the literature doesn't explicitly state the field's expectations, so watch talks (or talk to experts) to find out how people are really thinking beyond what they write in their papers.
          </p>
          
          <h4>Make predictions about your data</h4>
          <p>Go through your every element of executive summary again, making predictions about what an expert in the field would expect to see.
          Would the field predict that typical data examples look the way they do?
          What would the field predict the shapes of the histograms to look like?
          What correlations would experts predict?
          For each prediction, note whether it holds up or is violated by the data.
          Results that match your predictions are great sanity checks, and help convey soundness.
          The violations, however, are the interesting bits, so long as they're not caused by uninteresting confounds.
          </p>
          
        <h4>Data can be made interesting through model comparison</h4>
          <p>If there's nothing obviously interesting (unlikely), either select a new dataset, or alternatively, take a model comparison approach.
              Sometimes the data is high quality but the most interesting phenomena are hidden below the surface.
              A tried-and-true approach to making data interesting is to identify two mutually exclusive models that are both reasonable in the field, and ask which one better explains the data (or which models explain which aspects of the data).
              This allows you to "triangulate" where the data live in a space of models.
              If this is your approach, think carefully through what reasonable models to compare, and how you would compare them against the data.
          </p>

          
        <h2>Quantify the interesting phenomena</h2>
          The easiest way forward is to find something obviously interesting in the data, and then just be quantitative about it.
          
          <h4>Do simple analyses</h4>
          <p>
              It can be tempting to use sophisticated methods to quantify your results, but it's usually easier and more robust to do the simple thing first. Besides, when it comes to writing the paper the simple methods will serve as an important reference point, either to guide the more sophisticated methods or to clarify why the simple method is insufficient. If you want to do something more sophisticated make sure you can really justify why it's a necessary approach. Waiting a bit before starting a sophisticated approach can also help streamline the eventual tasks involved, since your unconscious may give you a lot of hints along the way before you actually sit down and start coding.
          </p>

          <h4>Make simple figures</h4>
          
          <p>
              A straightforward figure includes one or a few examples of the interesting phenomenon in the data, a schematic showing how you quantify an effect size or metric from the raw data, and a familiar plot (e.g. histograms, correlations, etc.) showing the effect size/metric across your dataset. Make sure to have clear text labels so that all elements of the figure are as unambiguous as possible.
          </p>

          <p>
              Simple figures are also crucial for presentations, even when you're not yet at the stage of writing up the paper. It may seem like unnecessary work to make nice schematics and add all the annotations for a lab meeting or committee meeting, but doing so will be extremely useful for your audience, since they all come with priors on what they expect to see in figures in their field. This can make the difference between having your presentation get derailed for an hour in a completely irrelevant direction, vs everyone understanding what you've done and providing genuinely helpful feedback.
          </p>

          <h4>Write why your results are interesting</h4>
          <p>For each interesting phenomenon, explain the context, and articulate what prediction or predictions an expert in the field would have made, ideally with rationale and citations. Then show how your results either violate the predictions or were not obviously predictable. Then explain the significance --- how the thinking in the field should be accordingly revised, according to what you've seen in the data.</p>

          <h4>Stay flexible</h4>
          <p>
              New questions may arise as the process continues.
              Research is highly iterative, and interesting data will be full of surprises.
              Keep a continually growing list of new ideas or curious observations, but hold off on diving in immediately.
              Letting ideas marinate for a while before execution can be extremely useful for pursuing them more thoughtfully and skillfully.
          </p>

          
        <h2>Beware the twilight zone</h2>
          <p>
          The <strong>twilight zone</strong> is when one applies a complex method to the data, not knowing exactly what to expect, then tries to interpret the results.
          Dimensionality reduction and clustering techniques are a common example.
          While these can sometimes be sound and powerful, and are often touted as "letting the data speak for itself", rather than going in with overly biased hypotheses, <strong>it is all too easy to mistake a feature of the data for a feature of the method</strong>.
          For instance, quantities like "number of clusters" are often a consequence of a parameter in the clustering algorithm (e.g. specifying cluster size), rather than a reflection of the "true" number of clusters in a dataset.
          </p>
          <p>
          The best way to verify results obtained in the twilight zone is to run the same methods on artificial datasets of the same format as your real data but in which more ground truth is known, which serve as controls.
          Understanding what your method returns for the artificial data is fundamental for interpreting the results when you apply it to the real data.
          This will give your results much more meaning and soundness, and will be crucial to convincing your audience that your results mean what you think they do.
          </p>
          
      </section>
        
    </main>

    <div id="footer-placeholder"></div>

    <script>
      fetch('/footer.html')
        .then(response => response.text())
        .then(data => {
          document.getElementById('footer-placeholder').innerHTML = data;
        })
        .catch(error => console.error('Error loading footer:', error));
    </script>
  </div>
</body>
</html>
